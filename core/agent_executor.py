#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Agentæ‰§è¡Œå™¨ - ä½¿ç”¨æ ‡å‡†æ¶ˆæ¯æ ¼å¼çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
å†å²åŠ¨ä½œé€šè¿‡ messages æ•°ç»„ä¼ é€’ï¼ˆè€Œé system_promptï¼‰ï¼Œæ”¯æŒå¤šæ¨¡æ€å›¾ç‰‡åµŒå…¥
"""
from typing import Dict, List
import sys
import json
import traceback
from collections import OrderedDict

# Windowså…¼å®¹æ€§ï¼šè®¾ç½®UTF-8ç¼–ç 
try:
    from utils.windows_compat import setup_console_encoding
    setup_console_encoding()
except ImportError:
    pass

from services.llm_client import SimpleLLMClient, ChatMessage
from core.context_builder import ContextBuilder
from core.tool_executor import ToolExecutor
from utils.conversation_storage import ConversationStorage
from utils.event_emitter import get_event_emitter as get_jsonl_emitter

from .agent_event_emitter import AgentEventEmitter
from .event_handlers import ConsoleLogHandler, JsonlStreamHandler
from .events import *
from utils.windows_compat import safe_print


class AgentExecutor:
    """Agentæ‰§è¡Œå™¨ - æ­£ç¡®çš„XMLä¸Šä¸‹æ–‡æ¶æ„"""
    
    def __init__(
        self,
        agent_name: str,
        agent_config: Dict,
        config_loader,
        hierarchy_manager
    ):
        """åˆå§‹åŒ–Agentæ‰§è¡Œå™¨"""
        self.agent_name = agent_name
        self.agent_config = agent_config
        self.config_loader = config_loader
        self.hierarchy_manager = hierarchy_manager
        
        self._setup_event_emitter()

        # ä»é…ç½®ä¸­æå–ä¿¡æ¯
        self.available_tools = agent_config.get("available_tools", [])
        self.max_turns = 10000000
        requested_model = agent_config.get("model_type", "claude-3-7-sonnet-20250219")
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = SimpleLLMClient()
        self.llm_client.set_tools_config(config_loader.all_tools)
        
        # éªŒè¯å¹¶è°ƒæ•´æ¨¡å‹
        available_models = self.llm_client.models
        final_model = requested_model
        is_fallback = False
        if requested_model not in available_models:
            final_model = available_models[0]
            is_fallback = True
        self.model_type = final_model
        
        # å‘é€æ¨¡å‹é€‰æ‹©äº‹ä»¶
        self.event_emitter.dispatch(ModelSelectionEvent(
            requested_model=requested_model,
            final_model=final_model,
            is_fallback=is_fallback
        ))

        # åˆå§‹åŒ–ä¸Šä¸‹æ–‡æ„é€ å™¨ï¼ˆè´Ÿè´£å®Œæ•´ä¸Šä¸‹æ–‡æ„å»ºï¼‰
        self.context_builder = ContextBuilder(
            hierarchy_manager,
            agent_config=agent_config,
            config_loader=config_loader,
            llm_client=self.llm_client,
            max_context_window=self.llm_client.max_context_window
        )
        
        # åˆå§‹åŒ–å·¥å…·æ‰§è¡Œå™¨
        self.tool_executor = ToolExecutor(config_loader, hierarchy_manager)
        
        # åˆå§‹åŒ–å¯¹è¯å­˜å‚¨
        self.conversation_storage = ConversationStorage()
        
        # AgentçŠ¶æ€
        self.agent_id = None
        self.action_history = []  # æ¸²æŸ“ç”¨ï¼ˆä¼šå‹ç¼©ï¼‰
        self.action_history_fact = []  # å®Œæ•´è½¨è¿¹ï¼ˆä¸å‹ç¼©ï¼‰
        self.pending_tools = []  # å¾…æ‰§è¡Œçš„å·¥å…·ï¼ˆç”¨äºæ¢å¤ï¼‰
        self.latest_thinking = ""
        self.first_thinking_done = False
        self.thinking_interval = 10  # æ¯10è½®å·¥å…·è°ƒç”¨è§¦å‘ä¸€æ¬¡thinking
        self.tool_call_counter = 0
        self.llm_turn_counter = 0  # LLMè°ƒç”¨è½®æ¬¡è®¡æ•°å™¨ï¼ˆç”¨äºæ¶ˆæ¯åˆ†ç»„ï¼‰

    def _setup_event_emitter(self):
        """åˆå§‹åŒ–äº‹ä»¶å‘å°„å™¨å¹¶æ³¨å†Œå¤„ç†å™¨"""
        self.event_emitter = AgentEventEmitter()
        self.event_emitter.register(ConsoleLogHandler())
        
        jsonl_emitter = get_jsonl_emitter()
        if jsonl_emitter.enabled:
            self.event_emitter.register(JsonlStreamHandler(enabled=True))
    
    def run(self, task_id: str, user_input: str) -> Dict:
        """æ‰§è¡ŒAgentä»»åŠ¡"""

        self.event_emitter.dispatch(AgentStartEvent(
            agent_name=self.agent_name, 
            task_input=user_input
        ))        
        # å­˜å‚¨ task_input ä¾›å‹ç¼©å™¨ä½¿ç”¨
        self.current_task_input = user_input

        # Agentå…¥æ ˆ
        self.agent_id = self.hierarchy_manager.push_agent(self.agent_name, user_input)

        # å°è¯•åŠ è½½å·²æœ‰çš„å¯¹è¯å†å²
        start_turn = self._load_state_from_storage(task_id)
        
        try:
            # é¦–æ¬¡thinkingï¼ˆåˆå§‹è§„åˆ’ï¼‰
            if start_turn == 0 and not self.first_thinking_done:
                thinking_result = self._trigger_thinking(
                    task_id, 
                    user_input, 
                    is_initial=True
                )
                if thinking_result:
                    self.latest_thinking = thinking_result
                    self.first_thinking_done = True
                    self.hierarchy_manager.update_thinking(self.agent_id, thinking_result)
                    self._save_state(task_id, user_input, 0)
        except Exception as e:
            self._handle_execution_error(e)
            # sys.exit(1) is called inside, so we don't need to return
        
        # å¼ºåˆ¶å·¥å…·è°ƒç”¨è®¡æ•°å™¨
        max_tool_try = 0

        # æ‰§è¡Œå¾ªç¯
        for turn in range(start_turn, self.max_turns):
            self.event_emitter.dispatch(CliDisplayEvent(
                message=f"\n--- ç¬¬ {turn + 1}/{self.max_turns} è½®æ‰§è¡Œ ---", 
                style='separator'
            ))
            
            try:
                # æ¯è½®å¼€å§‹å‰ä¿å­˜çŠ¶æ€
                self._save_state(task_id, user_input, turn)

                # æ£€æŸ¥å¹¶å‹ç¼©å†å²åŠ¨ä½œï¼ˆå¦‚æœè¶…è¿‡é™åˆ¶ï¼‰
                self._compress_action_history_if_needed()

                # æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å«å†å²åŠ¨ä½œï¼Œå†å²åŠ¨ä½œæ”¹ç”± messages æ‰¿è½½ï¼‰
                full_system_prompt = self.context_builder.build_context(
                    task_id,
                    self.agent_id,
                    self.agent_name,
                    user_input,
                    action_history=self.action_history,
                    include_action_history=False  # å†å²åŠ¨ä½œé€šè¿‡ messages ä¼ é€’
                )
                
                # ä» action_history æ„å»ºæ ‡å‡† messages æ•°ç»„
                messages = self._build_messages_from_action_history()
                
                # è°ƒç”¨LLMï¼ˆä½¿ç”¨æ ‡å‡† messages æ ¼å¼ï¼‰
                llm_response = self._execute_llm_call(full_system_prompt, messages)
                
                if llm_response.status != "success":
                    error_result = {
                        "status": "error",
                        "output": "LLMè°ƒç”¨å¤±è´¥",
                        "error_information": llm_response.error_information
                    }
                    self.hierarchy_manager.pop_agent(self.agent_id, str(error_result))
                    self.event_emitter.dispatch(AgentEndEvent(status='error', result=error_result))
                    return error_result

                if not llm_response.tool_calls:
                    # å¼ºåˆ¶å·¥å…·è°ƒç”¨æœºåˆ¶

                    if max_tool_try < 5:
                        max_tool_try += 1
                        self.event_emitter.dispatch(CliDisplayEvent(
                            message=f"âš ï¸ LLMæœªè°ƒç”¨å·¥å…·ï¼Œç¬¬{max_tool_try}/5æ¬¡æé†’", 
                            style='warning'
                        ))
                        self.action_history.append({
                            "_turn": self.llm_turn_counter,
                            "tool_name": "_no_tool_call",
                            "arguments": {},
                            "result": {
                                "status": "error",
                                "output": f"ç¬¬{max_tool_try}æ¬¡ï¼šLLMæœªè°ƒç”¨å·¥å…·ï¼Œè¯·åœ¨ä¸‹ä¸€è½®ä¸­å¿…é¡»è°ƒç”¨å·¥å…·"
                            },
                            "assistant_content": llm_response.output or ""
                        })
                        self.llm_turn_counter += 1
                        self._save_state(task_id, user_input, turn)
                        continue
                    else:
                        # 5æ¬¡åä»ä¸è°ƒç”¨ï¼Œè§¦å‘thinkingå¹¶æŠ¥é”™
                        thinking_result = self._trigger_thinking(
                            task_id, 
                            user_input, 
                            is_initial=False, 
                            is_forced=True
                        )
                        error_output = thinking_result or "å¤šæ¬¡æœªè°ƒç”¨å·¥å…·"
                        error_result = {
                            "status": "error",
                            "output": error_output,
                            "error_information": "Agentæ‹’ç»è°ƒç”¨å·¥å…·"
                        }
                        self.hierarchy_manager.pop_agent(self.agent_id, str(error_result))
                        self.event_emitter.dispatch(AgentEndEvent(status='error', result=error_result))
                        self.event_emitter.dispatch(ThinkingFailEvent(agent_name=self.agent_name, error_message=f"[{self.agent_name}] å¼ºåˆ¶thinking: {thinking_result if thinking_result else 'åˆ†æå¤±è´¥'}"))
                        return error_result
                # é‡ç½®è®¡æ•°å™¨ï¼ˆæˆåŠŸè°ƒç”¨äº†å·¥å…·ï¼‰
                max_tool_try = 0

                # æå–æœ¬è½® LLM è¾“å‡ºçš„æ–‡æœ¬å†…å®¹å’Œæ¨ç†å†…å®¹ï¼ˆæ‰€æœ‰ tool_call å…±äº«ï¼‰
                current_assistant_content = llm_response.output or ""
                current_reasoning_content = llm_response.reasoning_content or ""
                current_llm_turn = self.llm_turn_counter

                # æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
                for tool_call in llm_response.tool_calls:
                    final_output_result = self._execute_tool_call(
                        tool_call, task_id, user_input, turn,
                        assistant_content=current_assistant_content,
                        reasoning_content=current_reasoning_content,
                        llm_turn=current_llm_turn
                    )
                    if final_output_result:
                        self.event_emitter.dispatch(AgentEndEvent(status='success', result=final_output_result))
                        self.hierarchy_manager.pop_agent(self.agent_id, final_output_result.get("output", ""))
                        return final_output_result
                
                self.llm_turn_counter += 1
                
                # æ£€æŸ¥æ˜¯å¦è¯¥è§¦å‘thinkingï¼ˆæ¯Nè½®å·¥å…·è°ƒç”¨ï¼‰
                # ç”¨æ•´é™¤åˆ¤æ–­æ˜¯å¦è·¨è¿‡äº† thinking_interval è¾¹ç•Œï¼ˆé¿å…å¤š tool_call è·³è¿‡è¾¹ç•Œå€¼ï¼‰
                counter_before = self.tool_call_counter - len(llm_response.tool_calls)
                crossed_boundary = (counter_before // self.thinking_interval) < (self.tool_call_counter // self.thinking_interval)
                if self.tool_call_counter > 0 and crossed_boundary:
                    thinking_result = self._trigger_thinking(task_id, user_input, is_initial=False)
                    if thinking_result:
                        self.latest_thinking = thinking_result
                        self.hierarchy_manager.update_thinking(self.agent_id, thinking_result)
                        self._save_state(task_id, user_input, turn)
                        self.action_history = []
                        self.llm_turn_counter = 0  # é‡ç½®è½®æ¬¡è®¡æ•°å™¨
            
            except Exception as e:
                self._handle_execution_error(e)
        timeout_result = {
            "status": "error",
            "output": f"æ‰§è¡Œè¶…è¿‡æœ€å¤§è½®æ¬¡é™åˆ¶: {self.max_turns}",
            "error_information": f"Max turns {self.max_turns} exceeded"
        }
        self.hierarchy_manager.pop_agent(self.agent_id, str(timeout_result))
        self.event_emitter.dispatch(AgentEndEvent(status='error', result=timeout_result))
        self.event_emitter.dispatch(CliDisplayEvent(
            message="\nâš ï¸ è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶: {self.max_turns}"
        ))
        
        return timeout_result

    def _load_state_from_storage(self, task_id: str) -> int:
        """ä»å­˜å‚¨åŠ è½½çŠ¶æ€, è¿”å›èµ·å§‹è½®æ¬¡."""
        loaded_data = self.conversation_storage.load_actions(task_id, self.agent_id)
        start_turn = 0
        
        if loaded_data:
            self.action_history = loaded_data.get("action_history", [])
            self.action_history_fact = loaded_data.get("action_history_fact", [])
            self.pending_tools = loaded_data.get("pending_tools", [])
            self.latest_thinking = loaded_data.get("latest_thinking", "")
            self.first_thinking_done = loaded_data.get("first_thinking_done", False)
            self.tool_call_counter = loaded_data.get("tool_call_counter", 0)
            self.llm_turn_counter = loaded_data.get("llm_turn_counter", 0)
            start_turn = loaded_data.get("current_turn", 0) + 1
            
            self.event_emitter.dispatch(HistoryLoadEvent(
                start_turn=start_turn,
                action_history_len=len(self.action_history),
                action_history_fact_len=len(self.action_history_fact),
                pending_tool_count=len(self.pending_tools)
            ))
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆï¼ˆæœ‰final_outputï¼‰
            for action in self.action_history_fact:
                if action.get("tool_name") == "final_output":
                    final_result = action.get("result", {})
                    self.event_emitter.dispatch(CliDisplayEvent(
                        message=f"\nâœ… ä»»åŠ¡å·²å®Œæˆï¼Œç›´æ¥è¿”å›ä¹‹å‰çš„final_outputç»“æœ\n   çŠ¶æ€: {final_result.get('status')}", 
                        style='success'
                    ))
                    return final_result
            
            # æ¢å¤pendingå·¥å…·ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.pending_tools:
                self._recover_pending_tools(task_id)

        return start_turn

    def _build_messages_from_action_history(self) -> List[Dict]:
        """
        ä» action_history åŠ¨æ€é‡å»º OpenAI æ ‡å‡†æ ¼å¼çš„ messages æ•°ç»„
        
        æ”¯æŒä¸‰ç§ action ç±»å‹ï¼š
        1. _historical_summary â†’ user æ¶ˆæ¯ï¼ˆå‹ç¼©åçš„å†å²æ‘˜è¦ï¼‰
        2. _no_tool_call â†’ assistant æ¶ˆæ¯ï¼ˆçº¯æ–‡æœ¬ï¼‰+ user æ¶ˆæ¯ï¼ˆæé†’ï¼‰
        3. æ™®é€š action â†’ æŒ‰ _turn åˆ†ç»„ä¸º assistant(tool_calls) + tool(results) + user(images)
        
        Returns:
            OpenAI æ ¼å¼çš„ messages åˆ—è¡¨
        """
        # åˆå§‹ user æ¶ˆæ¯
        messages = [{
            "role": "user", 
            "content": "è¯·æ ¹æ®å½“å‰ä»»åŠ¡å’Œä¸Šä¸‹æ–‡ï¼Œæ‰§è¡Œä¸‹ä¸€æ­¥æ“ä½œã€‚è¯·è°ƒç”¨åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚ä¸è¦é‡å¤å·²æ‰§è¡Œçš„åŠ¨ä½œï¼"
        }]
        
        if not self.action_history:
            return messages
        
        # æŒ‰ _turn åˆ†ç»„æ™®é€š action
        turns = OrderedDict()
        
        for action in self.action_history:
            tool_name = action.get("tool_name", "")
            
            # ç‰¹æ®Šå¤„ç†ï¼šå†å²æ‘˜è¦ï¼ˆå‹ç¼©äº§ç‰©ï¼‰
            if tool_name == "_historical_summary":
                messages.append({
                    "role": "user",
                    "content": f"[Previous actions summary]\n{action['result']['output']}"
                })
                continue
            
            # ç‰¹æ®Šå¤„ç†ï¼šLLM æœªè°ƒç”¨å·¥å…·
            if tool_name == "_no_tool_call":
                assistant_content = action.get("assistant_content", "")
                if assistant_content:
                    messages.append({"role": "assistant", "content": assistant_content})
                messages.append({
                    "role": "user",
                    "content": action["result"].get("output", "è¯·è°ƒç”¨å·¥å…·")
                })
                continue
            
            # æ™®é€š action - æŒ‰ _turn åˆ†ç»„
            turn = action.get("_turn", 0)  # å‘åå…¼å®¹ï¼šæ—§è®°å½•é»˜è®¤ turn=0
            
            if turn not in turns:
                turns[turn] = {
                    "assistant_content": action.get("assistant_content", ""),
                    "reasoning_content": action.get("reasoning_content", ""),
                    "tool_calls": [],
                    "tool_results": [],
                    "images": []
                }
            
            # æ„å»º tool_call æ¡ç›®
            tool_call_id = action.get("tool_call_id", f"call_{turn}_{len(turns[turn]['tool_calls'])}")
            turns[turn]["tool_calls"].append({
                "id": tool_call_id,
                "type": "function",
                "function": {
                    "name": action["tool_name"],
                    "arguments": json.dumps(action["arguments"], ensure_ascii=False)
                }
            })
            
            # æ„å»º tool result æ¶ˆæ¯
            has_image = action.get("_has_image", False)
            has_base64 = bool(action.get("_image_base64"))
            
            if has_image and has_base64:
                # æœ‰å›¾ç‰‡ä¸”æœ‰ base64 æ•°æ® â†’ tool result ç®€çŸ­è¯´æ˜ï¼Œå›¾ç‰‡åœ¨åç»­ user æ¶ˆæ¯ä¸­åµŒå…¥
                result_content = "Image loaded successfully. See below."
            else:
                # æ— å›¾ç‰‡ æˆ– æœ‰å›¾ç‰‡æ ‡è®°ä½† base64 ä¸¢å¤±ï¼ˆCtrl+C æ¢å¤åœºæ™¯ï¼‰â†’ æ­£å¸¸ JSON ç»“æœ
                # æ’é™¤ _image_base64 ç­‰å†…éƒ¨å­—æ®µ
                result_clean = {k: v for k, v in action.get("result", {}).items() 
                               if not k.startswith("_")}
                result_content = json.dumps(result_clean, ensure_ascii=False)
            
            turns[turn]["tool_results"].append({
                "role": "tool",
                "tool_call_id": tool_call_id,
                "content": result_content
            })
            
            # æ”¶é›†å›¾ç‰‡æ•°æ®ï¼ˆæ–¹æ¡ˆäºŒï¼šåç»­ user æ¶ˆæ¯åµŒå…¥ï¼‰
            # åªæœ‰åŒæ—¶æœ‰ _has_image æ ‡è®°å’Œå®é™… base64 æ•°æ®æ—¶æ‰åµŒå…¥å›¾ç‰‡
            if has_image and has_base64:
                query = action.get("arguments", {}).get("query", "è¯·åˆ†æè¿™äº›å›¾ç‰‡")
                img_data = action["_image_base64"]
                # å…¼å®¹åˆ—è¡¨å’Œå•å€¼
                if isinstance(img_data, list):
                    base64_list = img_data
                else:
                    base64_list = [img_data]
                turns[turn]["images"].append({
                    "base64_list": base64_list,
                    "query": query
                })
        
        # ä»åˆ†ç»„æ•°æ®æ„å»º messages
        for turn_num in sorted(turns.keys()):
            turn_data = turns[turn_num]
            
            # assistant æ¶ˆæ¯ï¼ˆåŒ…å« contentã€tool_callsã€reasoning_contentï¼‰
            assistant_msg = {
                "role": "assistant",
                "content": turn_data["assistant_content"] or None,
                "tool_calls": turn_data["tool_calls"]
            }
            # å¦‚æœæœ‰ reasoning_contentï¼Œæ·»åŠ åˆ° assistant æ¶ˆæ¯ä¸­
            # LiteLLM ä¼šå°†å…¶ä¼ é€’ç»™æ”¯æŒ thinking çš„æ¨¡å‹ï¼ˆå¦‚ Anthropic Claudeï¼‰
            if turn_data.get("reasoning_content"):
                assistant_msg["reasoning_content"] = turn_data["reasoning_content"]
            messages.append(assistant_msg)
            
            # tool result æ¶ˆæ¯ï¼ˆæ¯ä¸ª tool_call å¯¹åº”ä¸€ä¸ªï¼‰
            messages.extend(turn_data["tool_results"])
            
            # å›¾ç‰‡æ¶ˆæ¯ï¼ˆæ–¹æ¡ˆäºŒï¼šè·Ÿåœ¨ tool result åé¢çš„ user æ¶ˆæ¯ï¼Œå¤šå¼ å›¾åˆå¹¶åˆ°ä¸€æ¡æ¶ˆæ¯ï¼‰
            for img_group in turn_data["images"]:
                content_parts = []
                for b64 in img_group["base64_list"]:
                    image_url = b64 if b64.startswith("data:") else f"data:image/jpeg;base64,{b64}"
                    content_parts.append({"type": "image_url", "image_url": {"url": image_url}})
                content_parts.append({
                    "type": "text",
                    "text": f"ä¸Šé¢æ˜¯ image_read è·å–çš„ {len(img_group['base64_list'])} å¼ å›¾ç‰‡ã€‚Agent çš„é—®é¢˜æ˜¯: {img_group['query']}"
                })
                messages.append({"role": "user", "content": content_parts})
        
        return messages

    def _execute_llm_call(self, system_prompt: str, messages: List[Dict] = None):
        """
        æ‰§è¡ŒLLMè°ƒç”¨å¹¶åˆ†å‘äº‹ä»¶
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆä¸å«å†å²åŠ¨ä½œï¼‰
            messages: OpenAI æ ‡å‡†æ ¼å¼çš„ messages æ•°ç»„ï¼ˆåŒ…å«å†å²åŠ¨ä½œï¼‰
        """
        if messages is None:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ä¼  messagesï¼Œä½¿ç”¨ç®€å•çš„ user æ¶ˆæ¯
            messages = [{"role": "user", "content": "è¯·è¾“å‡ºä¸‹ä¸€ä¸ªåŠ¨ä½œ"}]
        
        # å‘é€LLMè°ƒç”¨å¼€å§‹äº‹ä»¶
        self.event_emitter.dispatch(LlmCallStartEvent(
            model=self.model_type, 
            system_prompt=system_prompt
        ))
        
        # è°ƒç”¨LLMï¼ˆé‡è¯•æœºåˆ¶å·²åœ¨ llm_client å†…éƒ¨å®ç°ï¼‰
        llm_response = self.llm_client.chat(
            history=messages,
            model=self.model_type,
            system_prompt=system_prompt,
            tool_list=self.available_tools,
            tool_choice="required"  # å¼ºåˆ¶å·¥å…·è°ƒç”¨
        )
        
        self.event_emitter.dispatch(LlmCallEndEvent(
            llm_output=llm_response.output, 
            tool_calls=llm_response.tool_calls
        ))
        return llm_response

    def _execute_tool_call(self, tool_call: Dict, task_id: str, user_input: str, turn: int,
                          assistant_content: str = "", reasoning_content: str = "",
                          llm_turn: int = 0) -> Dict:
        """
        æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨å¹¶åˆ†å‘äº‹ä»¶
        
        Args:
            tool_call: å·¥å…·è°ƒç”¨å¯¹è±¡ï¼ˆåŒ…å« id, name, argumentsï¼‰
            task_id: ä»»åŠ¡ID
            user_input: ç”¨æˆ·è¾“å…¥
            turn: å½“å‰æ‰§è¡Œè½®æ¬¡
            assistant_content: è¯¥è½® LLM å“åº”çš„æ–‡æœ¬å†…å®¹ï¼ˆåŒè½®æ‰€æœ‰ tool_call å…±äº«ï¼‰
            reasoning_content: è¯¥è½® LLM å“åº”çš„æ¨ç†/æ€è€ƒå†…å®¹ï¼ˆåŒè½®æ‰€æœ‰ tool_call å…±äº«ï¼‰
            llm_turn: LLM è°ƒç”¨è½®æ¬¡ï¼ˆç”¨äºæ¶ˆæ¯åˆ†ç»„ï¼‰
        """
        # âœ… åœ¨ä¿å­˜ pending ä¹‹å‰ï¼Œä¸º level != 0 çš„å·¥å…·æ·»åŠ  uuid
        arguments_with_uuid = self._add_uuid_if_needed(tool_call.name, tool_call.arguments)
        
        # âœ… å…ˆæ ‡è®°ä¸ºpendingï¼ˆä¿å­˜å¸¦ uuid çš„å‚æ•°ï¼‰
        # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹äº‹ä»¶
        self.event_emitter.dispatch(ToolCallStartEvent(
            tool_name=tool_call.name, 
            arguments=arguments_with_uuid
        ))

        pending_tool = {
            "id": tool_call.id,
            "name": tool_call.name,
            "arguments": arguments_with_uuid,
            "status": "pending"
        }
        self.pending_tools.append(pending_tool)
        self._save_state(task_id, user_input, turn)  # ä¿å­˜pendingçŠ¶æ€

        # æ‰§è¡Œå·¥å…·ï¼ˆä½¿ç”¨å¸¦ uuid çš„å‚æ•°ï¼‰
        tool_result = self.tool_executor.execute(
            tool_call.name,
            arguments_with_uuid,
            task_id
        )

        # âœ… æ‰§è¡Œåä»pendingç§»é™¤
        self.pending_tools = [t for t in self.pending_tools if t["id"] != tool_call.id]
        
        # å‘é€å·¥å…·ç»“æœäº‹ä»¶
        self.event_emitter.dispatch(ToolCallEndEvent(
            tool_name=tool_call.name, 
            status=tool_result.get('status', 'unknown'), 
            result=tool_result
        ))

        # è®°å½•åŠ¨ä½œåˆ°å†å²ï¼ˆå¢å¼ºæ ¼å¼ï¼šåŒ…å«æ¶ˆæ¯é‡å»ºæ‰€éœ€çš„å­—æ®µï¼‰
        action_record = {
            "_turn": llm_turn,
            "tool_call_id": tool_call.id,
            "tool_name": tool_call.name,
            "arguments": arguments_with_uuid,
            "result": tool_result,
            "assistant_content": assistant_content,
            "reasoning_content": reasoning_content,  # æ¨¡å‹çš„æ¨ç†/æ€è€ƒå†…å®¹
            "_has_image": False,
            "_image_base64": None
        }
        
        # å¤„ç† image_read å·¥å…·è¿”å›ï¼ˆæ— è®º multimodal è®¾ç½®å¦‚ä½•ï¼Œéƒ½è¦æ¸…ç† base64ï¼‰
        if tool_call.name == "image_read":
            image_base64_list = None
            
            # ToolServer çš„ _call_toolserver ä¼šæŠŠå·¥å…·è¿”å›å€¼ json.dumps åˆ° output å­—ç¬¦ä¸²ä¸­
            # æ‰€ä»¥ _image_base64_list å¯èƒ½åµŒå¥—åœ¨ output å­—ç¬¦ä¸²é‡Œï¼Œéœ€è¦è§£ææå–
            output_str = tool_result.get("output", "")
            if isinstance(output_str, str) and ("_image_base64_list" in output_str or "_image_base64" in output_str):
                try:
                    inner_result = json.loads(output_str)
                    # æ–°æ ¼å¼ï¼š_image_base64_listï¼ˆæ•°ç»„ï¼‰
                    image_base64_list = inner_result.get("_image_base64_list")
                    # å…¼å®¹æ—§æ ¼å¼ï¼š_image_base64ï¼ˆå•å€¼ï¼‰â†’ è½¬ä¸ºåˆ—è¡¨
                    if not image_base64_list:
                        single = inner_result.get("_image_base64")
                        if single:
                            image_base64_list = [single]
                    
                    # ä» output ä¸­ç§»é™¤æ‰€æœ‰ base64 æ•°æ®
                    inner_result.pop("_image_base64_list", None)
                    inner_result.pop("_image_base64", None)
                    inner_result.pop("_multimodal", None)
                    tool_result["output"] = json.dumps(inner_result, indent=2, ensure_ascii=False)
                    action_record["result"] = tool_result
                except (json.JSONDecodeError, TypeError):
                    pass
            
            # ä¹Ÿæ£€æŸ¥é¡¶å±‚ï¼ˆä»¥é˜² ToolServer æœªåŒé‡åŒ…è£…ï¼‰
            if not image_base64_list:
                top_list = tool_result.get("_image_base64_list")
                top_single = tool_result.get("_image_base64")
                if top_list:
                    image_base64_list = top_list
                elif top_single:
                    image_base64_list = [top_single]
                tool_result.pop("_image_base64_list", None)
                tool_result.pop("_image_base64", None)
                tool_result.pop("_multimodal", None)
                action_record["result"] = tool_result
            
            # åªæœ‰å½“ä¸»æ¨¡å‹æ”¯æŒå¤šæ¨¡æ€æ—¶ï¼Œæ‰å°†å›¾ç‰‡åµŒå…¥ messages
            if image_base64_list and self.llm_client.multimodal:
                action_record["_has_image"] = True
                action_record["_image_base64"] = image_base64_list  # ç°åœ¨æ˜¯åˆ—è¡¨
        
        # æ·»åŠ åˆ°å®Œæ•´è½¨è¿¹ï¼ˆæ°¸ä¸å‹ç¼©ï¼Œä½†ä¸å­˜å‚¨ base64 ä»¥èŠ‚çœç©ºé—´ï¼‰
        fact_record = {k: v for k, v in action_record.items() if k != "_image_base64"}
        fact_record["_image_base64"] = None  # fact ä¸­ä¸ä¿ç•™ base64ï¼Œä»…è®°å½• _has_image æ ‡å¿—
        self.action_history_fact.append(fact_record)

        # æ·»åŠ åˆ°æ¸²æŸ“å†å²ï¼ˆä¼šè¢«å‹ç¼©ï¼Œä¿ç•™ base64 ç”¨äº messages é‡å»ºï¼‰
        self.action_history.append(action_record)

        self.hierarchy_manager.add_action(self.agent_id, {
            "tool_name": tool_call.name,
            "arguments": arguments_with_uuid,
            "result": {k: v for k, v in tool_result.items() if not k.startswith("_")}
        })

        # å·¥å…·æ‰§è¡Œåä¿å­˜çŠ¶æ€
        self._save_state(task_id, user_input, turn)
        
        # å¢åŠ å·¥å…·è°ƒç”¨è®¡æ•°
        self.tool_call_counter += 1
        
        # å¦‚æœæ˜¯final_outputï¼Œè¿”å›ç»“æœ
        if tool_call.name == "final_output":
            return tool_result
        return None

    def _handle_execution_error(self, e: Exception):
        """ç»Ÿä¸€å¤„ç†æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å¼‚å¸¸"""
        # è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
        error_type = type(e).__name__
        error_msg = str(e)
        error_traceback = traceback.format_exc()
        
        # æ„å»ºå‹å¥½çš„é”™è¯¯æç¤ºæ¶ˆæ¯
        error_display = f"""
âŒ æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä»»åŠ¡å·²ä¸­æ–­
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”´ é”™è¯¯ç±»å‹: {error_type}
ğŸ“ é”™è¯¯ä¿¡æ¯: {error_msg}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“‹ è¯¦ç»†å †æ ˆ:
{error_traceback}
"""
        
        # æ·»åŠ å½“å‰è¿›åº¦ä¿¡æ¯
        if self.latest_thinking:
            error_display += f"\nğŸ’­ å½“å‰è¿›åº¦:\n{self.latest_thinking[:500]}\n"
        
        error_display += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ ä»»åŠ¡å·²ä¿å­˜åœ¨å½“å‰çŠ¶æ€ï¼Œè¯·:
   1. æ ¹æ®é”™è¯¯ä¿¡æ¯æ’æŸ¥é—®é¢˜ï¼ˆä¿®å¤ç½‘ç»œã€é…ç½®ç­‰ï¼‰
   2. é‡æ–°å¯åŠ¨ CLI å¹¶è¾“å…¥ /resume å‘½ä»¤æ¢å¤ä»»åŠ¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        # é€šè¿‡äº‹ä»¶å‘é€é”™è¯¯
        self.event_emitter.dispatch(ErrorEvent(error_display=error_display))
        # ç›´æ¥é€€å‡ºç¨‹åº
        sys.exit(1)

    def _add_uuid_if_needed(
            self, 
            tool_name: str, 
            arguments: Dict
        ) -> Dict:
        """
        ä¸º level != 0 çš„å·¥å…·æ·»åŠ  uuid åç¼€åˆ° task_input
        
        Args:
            tool_name: å·¥å…·åç§°
            arguments: åŸå§‹å‚æ•°
            
        Returns:
            å¤„ç†åçš„å‚æ•°ï¼ˆå¦‚æœéœ€è¦æ·»åŠ  uuidï¼Œè¿”å›æ–°å­—å…¸ï¼›å¦åˆ™è¿”å›åŸå­—å…¸ï¼‰
        """
        try:
            # è·å–å·¥å…·é…ç½®
            tool_config = self.config_loader.get_tool_config(tool_name)
            tool_level = tool_config.get("level", 0)
            tool_type = tool_config.get("type", "")
            
            # åªå¯¹ level != 0 çš„ llm_call_agent æ·»åŠ  uuid
            if tool_type == "llm_call_agent" and tool_level != 0 and "task_input" in arguments:
                import uuid
                # åˆ›å»ºæ–°å­—å…¸ï¼ˆé¿å…ä¿®æ”¹åŸå§‹å‚æ•°ï¼‰
                new_arguments = arguments.copy()
                original_input = arguments["task_input"]
                random_suffix = f" [call-{uuid.uuid4().hex[:8]}]"
                new_arguments["task_input"] = original_input + random_suffix
                self.event_emitter.dispatch(CliDisplayEvent(
                    message=f"   ğŸ”– ä¸º level {tool_level} å·¥å…·æ·»åŠ  uuid åç¼€", 
                    style='info'
                ))
                return new_arguments
            
            # å…¶ä»–æƒ…å†µè¿”å›åŸå‚æ•°
            return arguments
        
        except Exception as e:
            self.event_emitter.dispatch(CliDisplayEvent(
                message=f"âš ï¸ æ·»åŠ  uuid æ—¶å‡ºé”™: {e}", 
                style='warning'
            ))
            return arguments
    
    def _trigger_thinking(self, task_id: str, task_input: str, is_initial: bool = False, is_forced: bool = False) -> str:
        """
        è§¦å‘Thinking Agentè¿›è¡Œåˆ†æ
        
        Args:
            task_id: ä»»åŠ¡ID
            task_input: ä»»åŠ¡è¾“å…¥
            is_initial: æ˜¯å¦æ˜¯é¦–æ¬¡thinking
            is_forced: æ˜¯å¦å› ä¸ºå¤šæ¬¡æœªè°ƒç”¨å·¥å…·è€Œè¢«å¼ºåˆ¶è§¦å‘thinking
            
        Returns:
            åˆ†æç»“æœ
        """
        # å‘é€Thinkingå¼€å§‹äº‹ä»¶
        self.event_emitter.dispatch(ThinkingStartEvent(
            agent_name=self.agent_name, 
            is_initial=is_initial, 
            is_forced=is_forced
        ))
        try:
            from services.thinking_agent import ThinkingAgent

            thinking_agent = ThinkingAgent()

            # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«å†å²åŠ¨ä½œXMLï¼Œä¾› thinking agent åˆ†æï¼‰
            full_system_prompt = self.context_builder.build_context(
                task_id,
                self.agent_id,
                self.agent_name,
                task_input,
                action_history=self.action_history,
                include_action_history=True  # thinking agent éœ€è¦çœ‹åˆ°å†å²åŠ¨ä½œ
            )
            result = thinking_agent.analyze_first_thinking(
                task_description=task_input,
                agent_system_prompt=full_system_prompt,
                available_tools=self.available_tools,
                tools_config=self.config_loader.all_tools,
                action_history=self.action_history,  # ä¼ é€’ action_historyï¼ˆå«å›¾ç‰‡æ•°æ®ï¼‰
                multimodal=self.llm_client.multimodal  # ä¼ é€’å¤šæ¨¡æ€æ ‡å¿—
            )
            # å‘é€ thinking äº‹ä»¶ï¼ˆå®Œæ•´å†…å®¹ï¼‰
            self.event_emitter.dispatch(ThinkingEndEvent(
                agent_name=self.agent_name, 
                result=result,
                is_initial=is_initial,
                is_forced=is_forced
            ))
            return result
        except Exception as e:
            error_msg = str(e)
            # å‘é€Thinkingå¤±è´¥äº‹ä»¶
            self.event_emitter.dispatch(ThinkingFailEvent(
                agent_name=self.agent_name, 
                error_message=error_msg
            ))
            raise Exception(str(e))

    def _compress_action_history_if_needed(self):
        """æ£€æŸ¥å¹¶å‹ç¼©å†å²åŠ¨ä½œï¼ˆå¦‚æœè¶…è¿‡ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼‰"""
        if not self.action_history:
            return
        
        try:
            from services.action_compressor import ActionCompressor

            # åˆå§‹åŒ–å‹ç¼©å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not hasattr(self, 'action_compressor'):
                self.action_compressor = ActionCompressor(self.llm_client)
            
            # ä½¿ç”¨æ–°çš„å‹ç¼©ç­–ç•¥ï¼ˆä¼ å…¥ thinking å’Œ task_inputï¼‰
            original_len = len(self.action_history)
            compressed = self.action_compressor.compress_if_needed(
                self.action_history,
                self.llm_client.max_context_window,
                thinking=self.latest_thinking,
                task_input=self.current_task_input
            )

            # å¦‚æœå‘ç”Ÿäº†å‹ç¼©ï¼Œæ›¿æ¢
            if len(compressed) < original_len:
                self.event_emitter.dispatch(CliDisplayEvent(
                    message=f"âœ… å†å²åŠ¨ä½œå·²å‹ç¼©: {original_len}æ¡ â†’ {len(compressed)}æ¡", 
                    style='success'
                ))
                self.action_history = compressed
        except Exception as e:
            self.event_emitter.dispatch(CliDisplayEvent(
                message=f"âš ï¸ å‹ç¼©å¤±è´¥: {e}", 
                style='warning'
            ))
            traceback.print_exc()
    
    def _recover_pending_tools(self, task_id: str):
        """æ¢å¤pendingçŠ¶æ€çš„å·¥å…·è°ƒç”¨"""
        for pending_tool in self.pending_tools:
            tool_name, tool_args = pending_tool['name'], pending_tool['arguments']
            try:
                self.event_emitter.dispatch(CliDisplayEvent(
                    message=f"   ğŸ”„ æ¢å¤æ‰§è¡Œ: {tool_name}\n   ğŸ“‹ å‚æ•°: {tool_args}", 
                    style='info'
                ))
                
                # é‡æ–°æ‰§è¡Œå·¥å…·
                tool_result = self.tool_executor.execute(
                    tool_name,
                    tool_args,
                    task_id
                )
                
                # è®°å½•ç»“æœ
                action_record = {
                    "tool_name": tool_name,
                    "arguments": tool_args,
                    "result": tool_result
                }
                
                self.action_history_fact.append(action_record)
                self.action_history.append(action_record)
                
                # ä»pendingç§»é™¤
                self.pending_tools.remove(pending_tool)
                
                self.event_emitter.dispatch(CliDisplayEvent(
                    message=f"   âœ… æ¢å¤å®Œæˆ: {tool_name}", 
                    style='success'
                ))
                
                # å¦‚æœæ˜¯final_outputï¼Œç›´æ¥è¿”å›
                if tool_name == "final_output":
                    return tool_result
            except Exception as e:
                self.event_emitter.dispatch(CliDisplayEvent(
                    message=f"   âŒ æ¢å¤å¤±è´¥: {tool_name} - {e}", 
                    style='error'
                ))
        # æ¸…ç©ºpendingåˆ—è¡¨
        self.pending_tools = []
    
    def _save_state(self, task_id: str, user_input: str, current_turn: int):
        """
        ä¿å­˜å½“å‰çŠ¶æ€
        
        Args:
            task_id: ä»»åŠ¡ID
            user_input: ç”¨æˆ·è¾“å…¥
            current_turn: å½“å‰è½®æ¬¡
        """
        # æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«å†å²åŠ¨ä½œXMLï¼Œç”¨äºè°ƒè¯•/å‚è€ƒï¼‰
        full_system_prompt = self.context_builder.build_context(
            task_id,
            self.agent_id,
            self.agent_name,
            user_input,
            action_history=self.action_history,
            include_action_history=True  # ä¿å­˜æ—¶åŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡
        )

        # ä¿å­˜çŠ¶æ€
        self.conversation_storage.save_actions(
            task_id=task_id,
            agent_id=self.agent_id,
            agent_name=self.agent_name,
            task_input=user_input,
            action_history=self.action_history,  # æ¸²æŸ“ç”¨ï¼ˆä¼šå‹ç¼©ï¼Œå« base64ï¼‰
            action_history_fact=self.action_history_fact,  # å®Œæ•´è½¨è¿¹ï¼ˆä¸å« base64ï¼‰
            pending_tools=self.pending_tools,
            current_turn=current_turn,
            latest_thinking=self.latest_thinking,
            first_thinking_done=self.first_thinking_done,
            tool_call_counter=self.tool_call_counter,
            llm_turn_counter=self.llm_turn_counter,
            system_prompt=full_system_prompt
        )


if __name__ == "__main__":
    from utils.config_loader import ConfigLoader
    from core.hierarchy_manager import get_hierarchy_manager
    
    # æµ‹è¯•
    config_loader = ConfigLoader("infiHelper")
    hierarchy_manager = get_hierarchy_manager("test_task")

    hierarchy_manager.start_new_instruction("æµ‹è¯•ä»»åŠ¡")

    # è·å–writing_agenté…ç½®
    agent_config = config_loader.get_tool_config("alpha_agent")

    safe_print(f"âœ… Agenté…ç½®: {agent_config.get('name')}")
    safe_print(f"   Tools: {len(agent_config.get('available_tools', []))}")
