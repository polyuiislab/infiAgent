#!/usr/bin/env python3
from utils.windows_compat import safe_print
# -*- coding: utf-8 -*-
"""
å†å²åŠ¨ä½œå‹ç¼©æœåŠ¡
ç­–ç•¥ï¼šæ€»ç»“å†å²XML + ä¿ç•™æœ€æ–°action + å‹ç¼©æœ€æ–°actionçš„å¤§å­—æ®µ
"""

import json
from typing import List, Dict

try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    HAS_TIKTOKEN = False


class ActionCompressor:
    """å†å²åŠ¨ä½œå‹ç¼©å™¨"""
    
    def __init__(self, llm_client):
        """
        åˆå§‹åŒ–
        
        Args:
            llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹ï¼ˆç”¨äºæ€»ç»“ï¼‰
        """
        self.llm_client = llm_client
        
        # åˆå§‹åŒ–tiktoken
        if HAS_TIKTOKEN:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        else:
            self.encoding = None
    
    def count_tokens(self, text: str) -> int:
        """ç»Ÿè®¡tokenæ•°"""
        if self.encoding:
            return len(self.encoding.encode(text))
        else:
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            other_chars = len(text) - chinese_chars
            return int(chinese_chars / 1.5 + other_chars / 4)
    
    def compress_if_needed(
        self,
        action_history: List[Dict],
        max_context_window: int,
        thinking: str = "",
        task_input: str = "",
        save_callback=None  # æ·»åŠ ä¿å­˜å›è°ƒï¼Œç¡®ä¿å‹ç¼©åç«‹å³ä¿å­˜
    ) -> List[Dict]:
        """
        æ£€æŸ¥å¹¶å‹ç¼©å†å²åŠ¨ä½œ
        
        ç­–ç•¥ï¼š
        1. ä¿ç•™æœ€æ–°1æ¡actionï¼ˆå®Œæ•´æˆ–å‹ç¼©å¤§å­—æ®µï¼‰
        2. ä¹‹å‰çš„æ‰€æœ‰actionæ€»ç»“ä¸ºä¸€ä¸ªsummary_action
        3. åŸºäº thinking å’Œ task_input åˆ¤æ–­å“ªäº›ä¿¡æ¯æœ‰æ•ˆã€å“ªäº›æ— å…³
        
        Args:
            action_history: åŠ¨ä½œå†å²
            max_context_window: æœ€å¤§çª—å£å¤§å°
            thinking: å½“å‰çš„ thinking å†…å®¹ï¼ˆåŒ…å« todolist å’Œè®¡åˆ’ï¼‰
            task_input: ä»»åŠ¡éœ€æ±‚æè¿°
            
        Returns:
            å‹ç¼©åçš„action_history
        """
        if not action_history:
            return []
        
        # å¦‚æœåªæœ‰ä¸€æ¡
        if len(action_history) == 1:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©å­—æ®µ
            return [self._compress_action_fields(action_history[0], max_context_window // 2)]
        
        # åˆ†ç¦»æœ€æ–°å’Œå†å²
        recent_action = action_history[-1]
        historical_actions = action_history[:-1]
        
        # è®¡ç®—æ•´ä½“tokenæ•°
        total_text = self._actions_to_xml(action_history)
        total_tokens = self.count_tokens(total_text+thinking+task_input)
        
        # å¦‚æœä¸è¶…é™ï¼Œä¸å‹ç¼©
        if total_tokens <= max_context_window - 20000:
            return action_history
        
        safe_print(f"ğŸ”„ å†å²åŠ¨ä½œéœ€è¦å‹ç¼©: {total_tokens} tokens > {max_context_window - 20000}")
        
        # å‹ç¼©ç­–ç•¥ï¼š
        # 1. å†å² â†’ åŸºäº thinking å’Œ task_input æ™ºèƒ½æ€»ç»“ä¸º5k tokens
        # 2. æœ€æ–° â†’ å‹ç¼©ä¸ºmax_windowçš„50%
        
        summary_action = self._summarize_historical_xml(
            self._actions_to_xml(historical_actions),
            target_tokens=5000,  # å†å²æ€»ç»“å›ºå®š5k tokens
            thinking=thinking,
            task_input=task_input,
            max_context_window=max_context_window
        )
        
        # å‹ç¼©æœ€æ–°actionçš„å¤§å­—æ®µï¼ˆ50% of max_windowï¼‰
        compressed_recent = self._compress_action_fields(
            recent_action,
            int(max_context_window * 0.5),  # 80000 * 0.5 = 40000 tokens
            thinking=thinking,
            task_input=task_input,
            max_context_window=max_context_window
        )
        
        result = [summary_action, compressed_recent]
        
        # éªŒè¯å‹ç¼©æ•ˆæœ
        result_xml = self._actions_to_xml(result)
        result_tokens = self.count_tokens(result_xml)
        safe_print(f"âœ… å‹ç¼©å®Œæˆ: {total_tokens} tokens â†’ {result_tokens} tokens (å‹ç¼©æ¯”: {result_tokens/total_tokens*100:.1f}%)")
        
        return result
    
    def _actions_to_xml(self, actions: List[Dict]) -> str:
        """å°†actionsè½¬æ¢ä¸ºXMLæ ¼å¼æ–‡æœ¬"""
        xml_parts = []
        for action in actions:
            tool_name = action.get("tool_name", "")
            arguments = action.get("arguments", {})
            result = action.get("result", {})
            
            action_xml = f"<action>\n  <tool_name>{tool_name}</tool_name>\n"
            
            # å‚æ•°
            for k, v in arguments.items():
                v_str = str(v).replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
                action_xml += f"  <tool_use:{k}>{v_str}</tool_use:{k}>\n"
            
            # ç»“æœ
            result_json = json.dumps(result, ensure_ascii=False, indent=2)
            action_xml += f"  <result>\n{result_json}\n  </result>\n</action>"
            
            xml_parts.append(action_xml)
        
        return "\n\n".join(xml_parts)
    
    def _summarize_historical_xml(
        self, 
        xml_text: str, 
        target_tokens: int = 5000,
        thinking: str = "",
        task_input: str = "",
        max_context_window: int = None
    ) -> Dict:
        """
        æ€»ç»“å†å²XMLå†…å®¹ä¸ºä¸€ä¸ªsummary action
        åŸºäº thinking å’Œ task_input æ™ºèƒ½åˆ¤æ–­å“ªäº›ä¿¡æ¯æœ‰æ•ˆ
        æ”¯æŒåˆ†æ®µå‹ç¼©ï¼šå¦‚æœæ•°æ®é‡è¿‡å¤§ï¼Œè‡ªåŠ¨åˆ†æ®µå¤„ç†
        
        Args:
            xml_text: å†å²actionsçš„XMLæ–‡æœ¬
            target_tokens: ç›®æ ‡tokenæ•°
            thinking: å½“å‰çš„ thinking å†…å®¹ï¼ˆåŒ…å« todolist å’Œè®¡åˆ’ï¼‰
            task_input: ä»»åŠ¡éœ€æ±‚æè¿°
            max_context_window: æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ®µï¼‰
            
        Returns:
            ä¸€ä¸ªsummary action
        """
        try:
            from services.llm_client import ChatMessage
            
            # æ£€æŸ¥æ•°æ®é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†æ®µå‹ç¼©
            xml_tokens = self.count_tokens(xml_text)
            
            # è·å–å‹ç¼©æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼ˆä»å‚æ•°æˆ–LLMå®¢æˆ·ç«¯è·å–ï¼‰
            compressor_context_limit = max_context_window or self.llm_client.max_context_window
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = ""
            if task_input:
                context_info += f"\n<ä»»åŠ¡éœ€æ±‚>\n{task_input}\n</ä»»åŠ¡éœ€æ±‚>\n"
            if thinking:
                context_info += f"\n<å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n{thinking}\n</å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n"
            
            context_tokens = self.count_tokens(context_info)
            
            # å¦‚æœæ•°æ®é‡ + ä¸Šä¸‹æ–‡ + æç¤ºè¯ è¶…è¿‡æ¨¡å‹é™åˆ¶çš„60%ï¼Œä½¿ç”¨åˆ†æ®µå‹ç¼©
            overhead_tokens = 2000  # æç¤ºè¯å’Œæ ¼å¼çš„å¼€é”€
            available_tokens = int(compressor_context_limit * 0.6) - context_tokens - overhead_tokens
            
            if xml_tokens > available_tokens:
                safe_print(f"   ğŸ“¦ æ•°æ®é‡è¿‡å¤§({xml_tokens} tokens)ï¼Œå¯ç”¨åˆ†æ®µå‹ç¼©")
                return self._chunked_summarize(xml_text, target_tokens, thinking, task_input, available_tokens)
            
            # æ•°æ®é‡åˆé€‚ï¼Œç›´æ¥å‹ç¼©
            return self._single_summarize(xml_text, target_tokens, thinking, task_input, context_info)
        
        except Exception as e:
            safe_print(f"âš ï¸ æ€»ç»“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                "tool_name": "_historical_summary",
                "arguments": {},
                "result": {"status": "success", "output": "[å†å²åŠ¨ä½œå·²çœç•¥]", "_is_summary": True}
            }
    
    def _single_summarize(
        self,
        xml_text: str,
        target_tokens: int,
        thinking: str,
        task_input: str,
        context_info: str
    ) -> Dict:
        """
        å•æ¬¡å‹ç¼©ï¼ˆæ•°æ®é‡ä¸å¤§æ—¶ä½¿ç”¨ï¼‰
        """
        from services.llm_client import ChatMessage
        
        prompt = f"""ä½ æ˜¯æ™ºèƒ½å†å²ä¿¡æ¯å‹ç¼©åŠ©æ‰‹ã€‚è¯·åŸºäºä»»åŠ¡éœ€æ±‚å’Œå½“å‰è¿›åº¦ï¼Œæ™ºèƒ½å‹ç¼©ä»¥ä¸‹å†å²åŠ¨ä½œã€‚

{context_info}

<å†å²åŠ¨ä½œ>
{xml_text}
</å†å²åŠ¨ä½œ>

å‹ç¼©è¦æ±‚ï¼š
1. **ç›®æ ‡é•¿åº¦**: ä¸¥æ ¼æ§åˆ¶åœ¨ {target_tokens} tokens ä»¥å†…
2. **æ™ºèƒ½ç­›é€‰**: 
   - åˆ†æ thinking ä¸­çš„ todolist/è®¡åˆ’ï¼Œåˆ¤æ–­å“ªäº›åŠ¨ä½œæ˜¯ä¸ºäº†å®Œæˆæœªå®Œæˆçš„ä»»åŠ¡ç›®æ ‡
   - ä¿ç•™å·²å®Œæˆä»»åŠ¡ç›¸å…³çš„**å…³é”®ç»“æœ**ï¼ˆå¦‚ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„ã€é‡è¦è¾“å‡ºï¼‰
   - ä¸¢å¼ƒæ— å…³æˆ–å¤±è´¥çš„å°è¯•ä¿¡æ¯
3. **ä¼˜å…ˆä¿ç•™**:
   - æˆåŠŸå®Œæˆçš„å…³é”®æ­¥éª¤ï¼ˆå¦‚åˆ›å»ºçš„æ–‡ä»¶ã€æ‰§è¡Œçš„ä»£ç ã€è·å–çš„æ•°æ®ï¼‰
   - é‡è¦çš„æ–‡ä»¶è·¯å¾„å’Œä½ç½®ä¿¡æ¯
   - å¯¹åç»­ä»»åŠ¡æœ‰å‚è€ƒä»·å€¼çš„è¾“å‡º
4. **å¯ä»¥ä¸¢å¼ƒ**:
   - é‡å¤çš„å°è¯•å’Œé”™è¯¯ä¿¡æ¯
   - ä¸­é—´çš„è°ƒè¯•è¿‡ç¨‹
   - ä¸å½“å‰ä»»åŠ¡ç›®æ ‡æ— å…³çš„æ¢ç´¢æ€§æ“ä½œ
5. **æ ¼å¼è¦æ±‚**:
   - æŒ‰æ—¶é—´é¡ºåºæ€»ç»“
   - çªå‡ºå…³é”®æˆæœå’Œäº§å‡º
   - ä¿æŒä¿¡æ¯çš„è¿è´¯æ€§

è¯·ç›´æ¥è¾“å‡ºå‹ç¼©åçš„æ€»ç»“ï¼ˆä¸­æ–‡ï¼‰ï¼š"""
        
        history = [ChatMessage(role="user", content=prompt)]
        
        response = self.llm_client.chat(
            history=history,
            model=self.llm_client.compressor_models[0],
            system_prompt=f"ä½ æ˜¯æ•´ä½“ä¸Šä¸‹æ–‡æ„é€ ä¸“å®¶ã€‚ç›®æ ‡ï¼šå°†å†…å®¹å‹ç¼©åˆ°{target_tokens} tokensä»¥å†…ã€‚",
            tool_list=[],  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
            tool_choice="none"  # æ˜ç¡®è¡¨ç¤ºä¸è°ƒç”¨å·¥å…·ï¼ˆå‹ç¼©ä»»åŠ¡ï¼‰
        )
        
        summary = response.output if response.status == "success" else "[æ€»ç»“å¤±è´¥]"
        
        return {
            "tool_name": "_historical_summary",
            "arguments": {},
            "result": {
                "status": "success",
                "output": summary,
                "_is_summary": True
            }
        }
    
    def _chunked_summarize(
        self,
        xml_text: str,
        target_tokens: int,
        thinking: str,
        task_input: str,
        chunk_size_tokens: int
    ) -> Dict:
        """
        åˆ†æ®µå‹ç¼©ï¼ˆæ•°æ®é‡è¿‡å¤§æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            xml_text: å®Œæ•´çš„XMLæ–‡æœ¬
            target_tokens: æœ€ç»ˆç›®æ ‡tokenæ•°
            thinking: thinkingå†…å®¹
            task_input: ä»»åŠ¡è¾“å…¥
            chunk_size_tokens: æ¯æ®µçš„æœ€å¤§tokenæ•°
        
        Returns:
            å‹ç¼©åçš„summary action
        """
        from services.llm_client import ChatMessage
        
        # æŒ‰actionåˆ†å‰²xml_text
        # ç®€å•æ–¹æ³•ï¼šæŒ‰ </action> åˆ†å‰²
        action_blocks = xml_text.split('</action>')
        action_blocks = [block + '</action>' for block in action_blocks if block.strip()]
        
        # å°†actionsåˆ†ç»„åˆ°chunksä¸­
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0
        
        for action_block in action_blocks:
            action_tokens = self.count_tokens(action_block)
            
            if current_chunk_tokens + action_tokens > chunk_size_tokens and current_chunk:
                # å½“å‰chunkå·²æ»¡ï¼Œå¼€å§‹æ–°chunk
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [action_block]
                current_chunk_tokens = action_tokens
            else:
                current_chunk.append(action_block)
                current_chunk_tokens += action_tokens
        
        # æ·»åŠ æœ€åä¸€ä¸ªchunk
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        safe_print(f"      åˆ†æˆ {len(chunks)} æ®µè¿›è¡Œå‹ç¼©")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if task_input:
            context_info += f"\n<ä»»åŠ¡éœ€æ±‚>\n{task_input}\n</ä»»åŠ¡éœ€æ±‚>\n"
        if thinking:
            context_info += f"\n<å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n{thinking}\n</å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n"
        
        # å¯¹æ¯ä¸ªchunkè¿›è¡Œå‹ç¼©
        chunk_summaries = []
        target_per_chunk = target_tokens // len(chunks)
        
        for i, chunk in enumerate(chunks):
            safe_print(f"      å‹ç¼©ç¬¬ {i+1}/{len(chunks)} æ®µ...")
            
            prompt = f"""ä½ æ˜¯æ™ºèƒ½å†å²ä¿¡æ¯å‹ç¼©åŠ©æ‰‹ã€‚è¿™æ˜¯åˆ†æ®µå‹ç¼©ä»»åŠ¡çš„ç¬¬ {i+1}/{len(chunks)} æ®µã€‚

{context_info}

<æœ¬æ®µå†å²åŠ¨ä½œ>
{chunk}
</æœ¬æ®µå†å²åŠ¨ä½œ>

å‹ç¼©è¦æ±‚ï¼š
1. **ç›®æ ‡é•¿åº¦**: ä¸¥æ ¼æ§åˆ¶åœ¨ {target_per_chunk} tokens ä»¥å†…
2. **æ™ºèƒ½ç­›é€‰**: 
   - æ ¹æ®ä»»åŠ¡éœ€æ±‚å’Œè¿›åº¦ï¼Œä¿ç•™å…³é”®ç»“æœå’Œé‡è¦ä¿¡æ¯
   - ä¸¢å¼ƒæ— å…³æˆ–å¤±è´¥çš„å°è¯•
3. **ä¼˜å…ˆä¿ç•™**:
   - æˆåŠŸçš„å…³é”®æ­¥éª¤å’Œäº§å‡º
   - é‡è¦çš„æ–‡ä»¶è·¯å¾„å’Œæ•°æ®
   - å¯¹åç»­ä»»åŠ¡æœ‰ä»·å€¼çš„è¾“å‡º
4. **æ ¼å¼è¦æ±‚**:
   - æŒ‰æ—¶é—´é¡ºåºç®€è¦æ€»ç»“æœ¬æ®µçš„å…³é”®åŠ¨ä½œ
   - çªå‡ºé‡è¦æˆæœ

è¯·ç›´æ¥è¾“å‡ºæœ¬æ®µçš„å‹ç¼©æ€»ç»“ï¼ˆä¸­æ–‡ï¼‰ï¼š"""
            
            history = [ChatMessage(role="user", content=prompt)]
            
            try:
                response = self.llm_client.chat(
                    history=history,
                    model=self.llm_client.compressor_models[0],
                    system_prompt=f"ä½ æ˜¯å†…å®¹å‹ç¼©ä¸“å®¶ã€‚ç›®æ ‡ï¼šå°†æœ¬æ®µå‹ç¼©åˆ°{target_per_chunk} tokensä»¥å†…ã€‚",
                    tool_list=[],  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
                    tool_choice="none"  # æ˜ç¡®è¡¨ç¤ºä¸è°ƒç”¨å·¥å…·ï¼ˆå‹ç¼©ä»»åŠ¡ï¼‰
                )
                
                if response.status == "success":
                    chunk_summaries.append(f"[æ®µ{i+1}] {response.output}")
                    safe_print(f"         âœ… ç¬¬{i+1}æ®µå‹ç¼©æˆåŠŸ")
                else:
                    chunk_summaries.append(f"[æ®µ{i+1}] [å‹ç¼©å¤±è´¥]")
                    safe_print(f"         âš ï¸ ç¬¬{i+1}æ®µå‹ç¼©å¤±è´¥: {response.output}")
            except Exception as e:
                chunk_summaries.append(f"[æ®µ{i+1}] [å‹ç¼©å¼‚å¸¸]")
                safe_print(f"         âŒ ç¬¬{i+1}æ®µå‹ç¼©å¼‚å¸¸: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ®µçš„æ€»ç»“
        final_summary = "\n\n".join(chunk_summaries)
        
        safe_print(f"      âœ… åˆ†æ®µå‹ç¼©å®Œæˆï¼Œå…±{len(chunks)}æ®µ")
        
        return {
            "tool_name": "_historical_summary",
            "arguments": {},
            "result": {
                "status": "success",
                "output": final_summary,
                "_is_summary": True,
                "_chunked": True,
                "_chunks_count": len(chunks)
            }
        }
    
    def _compress_action_fields(
        self, 
        action: Dict, 
        max_field_tokens: int,
        thinking: str = "",
        task_input: str = "",
        max_context_window: int = None
    ) -> Dict:
        """
        å‹ç¼©actionä¸­çš„å¤§å­—æ®µï¼ˆargumentså’Œresultï¼‰
        
        Args:
            action: åŸå§‹action
            max_field_tokens: å•ä¸ªå­—æ®µçš„æœ€å¤§tokenæ•°ï¼ˆé€šå¸¸æ˜¯max_context_window/2ï¼‰
            thinking: å½“å‰çš„ thinking å†…å®¹
            task_input: ä»»åŠ¡éœ€æ±‚æè¿°
            max_context_window: æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ï¼ˆä¼ é€’ç»™å­—æ®µå‹ç¼©æ–¹æ³•ï¼‰
            
        Returns:
            å‹ç¼©åçš„action
        """
        compressed_action = action.copy()
        
        # å‹ç¼©argumentsä¸­çš„å¤§å­—æ®µ
        if "arguments" in compressed_action:
            compressed_args = {}
            for k, v in compressed_action["arguments"].items():
                v_str = str(v)
                v_tokens = self.count_tokens(v_str)
                
                if v_tokens > max_field_tokens:
                    safe_print(f"   ğŸ¤– LLMå‹ç¼©arguments.{k}: {v_tokens} tokens â†’ {max_field_tokens} tokens")
                    compressed_v = self._llm_compress_field(
                        v_str, 
                        max_field_tokens, 
                        action.get("tool_name", "unknown"),
                        thinking=thinking,
                        task_input=task_input,
                        field_context=f"å·¥å…· '{action.get('tool_name')}' çš„å‚æ•° '{k}'",
                        max_context_window=max_context_window
                    )
                    compressed_args[k] = compressed_v
                else:
                    compressed_args[k] = v
            compressed_action["arguments"] = compressed_args
        
        # å‹ç¼©result.output
        if "result" in compressed_action and "output" in compressed_action["result"]:
            output = compressed_action["result"]["output"]
            output_tokens = self.count_tokens(output)
            
            if output_tokens > max_field_tokens:
                safe_print(f"   ğŸ¤– LLMå‹ç¼©result.output: {output_tokens} tokens â†’ {max_field_tokens} tokens")
                # æ„å»ºå­—æ®µä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å·¥å…·å‚æ•°ä¿¡æ¯ï¼‰
                args_summary = ", ".join([f"{k}={v}" for k, v in compressed_action.get("arguments", {}).items()])
                field_context = f"å·¥å…· '{action.get('tool_name')}' çš„æ‰§è¡Œç»“æœ (å‚æ•°: {args_summary})"
                
                compressed_output = self._llm_compress_field(
                    output, 
                    max_field_tokens, 
                    action.get("tool_name", "unknown"),
                    thinking=thinking,
                    task_input=task_input,
                    field_context=field_context,
                    max_context_window=max_context_window
                )
                compressed_action["result"]["output"] = compressed_output
                compressed_action["result"]["_compressed"] = True
                compressed_action["result"]["_original_tokens"] = output_tokens
        
        return compressed_action
    
    def _llm_compress_field(
        self, 
        text: str, 
        target_tokens: int, 
        tool_name: str,
        thinking: str = "",
        task_input: str = "",
        field_context: str = "",
        max_context_window: int = None
    ) -> str:
        """
        ä½¿ç”¨LLMæ™ºèƒ½å‹ç¼©å•ä¸ªå­—æ®µ
        æ”¯æŒåˆ†æ®µå‹ç¼©ï¼šå¦‚æœå­—æ®µå†…å®¹è¿‡å¤§ï¼Œè‡ªåŠ¨åˆ†æ®µå¤„ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            target_tokens: ç›®æ ‡tokenæ•°
            tool_name: å·¥å…·åç§°ï¼ˆç”¨äºä¼˜åŒ–æç¤ºè¯ï¼‰
            thinking: å½“å‰çš„ thinking å†…å®¹
            task_input: ä»»åŠ¡éœ€æ±‚æè¿°
            field_context: å­—æ®µä¸Šä¸‹æ–‡ï¼ˆå¦‚ "å·¥å…· 'file_read' çš„å‚æ•° 'path'"ï¼‰
            max_context_window: æœ€å¤§ä¸Šä¸‹æ–‡çª—å£ï¼ˆç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ®µï¼‰
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        try:
            from services.llm_client import ChatMessage
            
            # æ ¹æ®å·¥å…·ç±»å‹å®šåˆ¶æç¤ºè¯
            if "parse" in tool_name.lower() or "read" in tool_name.lower():
                content_type = "æ–‡æ¡£å†…å®¹"
                focus = "ä¿ç•™æ–‡æ¡£çš„å…³é”®ç« èŠ‚ã€æ ¸å¿ƒè®ºç‚¹ã€é‡è¦æ•°æ®å’Œç»“è®º"
            elif "execute" in tool_name.lower() or "run" in tool_name.lower():
                content_type = "ä»£ç æ‰§è¡Œç»“æœ"
                focus = "ä¿ç•™å…³é”®è¾“å‡ºã€é”™è¯¯ä¿¡æ¯ã€è¿”å›å€¼å’Œæ‰§è¡ŒçŠ¶æ€"
            elif "search" in tool_name.lower():
                content_type = "æœç´¢ç»“æœ"
                focus = "ä¿ç•™æœ€ç›¸å…³çš„æœç´¢ç»“æœå’Œå…³é”®åŒ¹é…ä¿¡æ¯"
            else:
                content_type = "å†…å®¹"
                focus = "ä¿ç•™æœ€é‡è¦çš„æ ¸å¿ƒä¿¡æ¯"
            
            # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_info = ""
            if task_input:
                context_info += f"\n<ä»»åŠ¡éœ€æ±‚>\n{task_input}\n</ä»»åŠ¡éœ€æ±‚>\n"
            if thinking:
                context_info += f"\n<å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n{thinking}\n</å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n"
            if field_context:
                context_info += f"\n<å­—æ®µæ¥æº>\nè¿™æ˜¯æœ€æ–°åŠ¨ä½œä¸­ {field_context} çš„å†…å®¹\n</å­—æ®µæ¥æº>\n"
            
            # æ£€æŸ¥å­—æ®µå¤§å°ï¼Œå†³å®šæ˜¯å¦éœ€è¦åˆ†æ®µå‹ç¼©
            text_tokens = self.count_tokens(text)
            context_tokens = self.count_tokens(context_info)
            
            # è·å–å‹ç¼©æ¨¡å‹çš„ä¸Šä¸‹æ–‡é™åˆ¶ï¼ˆä»å‚æ•°æˆ–LLMå®¢æˆ·ç«¯è·å–ï¼‰
            compressor_context_limit = max_context_window or self.llm_client.max_context_window
            overhead_tokens = 1000  # æç¤ºè¯å¼€é”€
            available_tokens = int(compressor_context_limit * 0.6) - context_tokens - overhead_tokens
            
            # å¦‚æœæ–‡æœ¬è¿‡å¤§ï¼Œä½¿ç”¨åˆ†æ®µå‹ç¼©
            if text_tokens > available_tokens:
                safe_print(f"      ğŸ“¦ å­—æ®µè¿‡å¤§({text_tokens} tokens)ï¼Œå¯ç”¨åˆ†æ®µå‹ç¼©")
                return self._chunked_compress_field(
                    text, target_tokens, tool_name, content_type, focus,
                    thinking, task_input, field_context, available_tokens
                )
            
            # æ–‡æœ¬å¤§å°åˆé€‚ï¼Œç›´æ¥å‹ç¼©
            prompt = f"""ä½ æ˜¯æ™ºèƒ½å†…å®¹å‹ç¼©åŠ©æ‰‹ã€‚è¯·åŸºäºä»»åŠ¡éœ€æ±‚å’Œå½“å‰è¿›åº¦ï¼Œå‹ç¼©ä»¥ä¸‹{content_type}ã€‚

{context_info}

<å¾…å‹ç¼©çš„{content_type}>
{text}
</å¾…å‹ç¼©çš„{content_type}>

å‹ç¼©è¦æ±‚ï¼š
1. **ç›®æ ‡é•¿åº¦**: ä¸¥æ ¼æ§åˆ¶åœ¨ {target_tokens} tokens ä»¥å†…
2. **æ™ºèƒ½ç­›é€‰**: 
   - æ ¹æ® thinking ä¸­çš„ä»»åŠ¡è¿›åº¦ï¼Œåˆ¤æ–­å“ªäº›ä¿¡æ¯å¯¹æœªå®Œæˆçš„ä»»åŠ¡æœ‰ä»·å€¼
   - {focus}
   - ä¸¢å¼ƒä¸å½“å‰ä»»åŠ¡ç›®æ ‡æ— å…³çš„å†…å®¹
3. **ä¼˜å…ˆä¿ç•™**:
   - ä¸ä»»åŠ¡ç›®æ ‡ç›´æ¥ç›¸å…³çš„å…³é”®ä¿¡æ¯
   - é‡è¦çš„æ–‡ä»¶è·¯å¾„ã€æ•°æ®ã€ç»“æœ
   - åç»­æ­¥éª¤éœ€è¦å¼•ç”¨çš„å†…å®¹
4. **å¯ä»¥ä¸¢å¼ƒ**:
   - å†—ä½™çš„ç»†èŠ‚å’Œé‡å¤ä¿¡æ¯
   - ä¸ä»»åŠ¡æ— å…³çš„æ¢ç´¢æ€§å†…å®¹
   - ä¸­é—´è¿‡ç¨‹çš„è°ƒè¯•ä¿¡æ¯
5. **æ ¼å¼è¦æ±‚**:
   - ä¿æŒä¿¡æ¯çš„è¿è´¯æ€§å’Œå¯è¯»æ€§
   - ä½¿ç”¨æ€»ç»“å’Œæç‚¼ï¼Œè€Œéç®€å•æˆªæ–­
   - å¦‚æœæœ‰ç»“æ„åŒ–å†…å®¹ï¼ˆè¡¨æ ¼ã€åˆ—è¡¨ï¼‰ï¼Œä¿ç•™å…³é”®éƒ¨åˆ†

è¯·ç›´æ¥è¾“å‡ºå‹ç¼©åçš„å†…å®¹ï¼ˆä¸è¦é¢å¤–è¯´æ˜ï¼‰ï¼š"""
            
            history = [ChatMessage(role="user", content=prompt)]
            
            response = self.llm_client.chat(
                history=history,
                model=self.llm_client.compressor_models[0],
                system_prompt=f"ä½ æ˜¯æ™ºèƒ½å†…å®¹å‹ç¼©åŠ©æ‰‹ã€‚ç›®æ ‡ï¼šå°†{content_type}å‹ç¼©åˆ°{target_tokens} tokensï¼ŒåŒæ—¶ä¿ç•™æ ¸å¿ƒä¿¡æ¯ã€‚",
                tool_list=[],  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
                tool_choice="none"  # æ˜ç¡®è¡¨ç¤ºä¸è°ƒç”¨å·¥å…·ï¼ˆå‹ç¼©ä»»åŠ¡ï¼‰
            )
            
            compressed = response.output if response.status == "success" else text[:1000] + "\n[å‹ç¼©å¤±è´¥ï¼Œä»…ä¿ç•™å‰1000å­—ç¬¦]"
            
            # éªŒè¯å‹ç¼©æ•ˆæœ
            actual_tokens = self.count_tokens(compressed)
            safe_print(f"      å‹ç¼©æ•ˆæœ: {actual_tokens}/{target_tokens} tokens ({actual_tokens/target_tokens*100:.1f}%)")
            
            return compressed
            
        except Exception as e:
            safe_print(f"âš ï¸ LLMå‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨fallback: {e}")
            # fallbackï¼šé¦–å°¾ä¿ç•™
            return self._fallback_compress(text, target_tokens)
    
    def _chunked_compress_field(
        self,
        text: str,
        target_tokens: int,
        tool_name: str,
        content_type: str,
        focus: str,
        thinking: str,
        task_input: str,
        field_context: str,
        chunk_size_tokens: int
    ) -> str:
        """
        åˆ†æ®µå‹ç¼©å­—æ®µå†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            target_tokens: æœ€ç»ˆç›®æ ‡tokenæ•°
            tool_name: å·¥å…·åç§°
            content_type: å†…å®¹ç±»å‹æè¿°
            focus: å‹ç¼©é‡ç‚¹
            thinking: thinkingå†…å®¹
            task_input: ä»»åŠ¡è¾“å…¥
            field_context: å­—æ®µä¸Šä¸‹æ–‡
            chunk_size_tokens: æ¯æ®µçš„æœ€å¤§tokenæ•°
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        from services.llm_client import ChatMessage
        
        # æŒ‰æ®µè½æˆ–å›ºå®šå­—ç¬¦æ•°åˆ†å‰²æ–‡æœ¬
        # ç®€å•ç­–ç•¥ï¼šæŒ‰\n\nåˆ†å‰²æ®µè½ï¼Œå¦‚æœæ®µè½å¤ªå¤§åˆ™æŒ‰å­—ç¬¦æ•°åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = []
        current_chunk_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡chunkå¤§å°ï¼Œéœ€è¦å¼ºåˆ¶åˆ†å‰²
            if para_tokens > chunk_size_tokens:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = []
                    current_chunk_tokens = 0
                
                # æŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²å¤§æ®µè½
                chars_per_chunk = int(chunk_size_tokens * 3)  # ç²—ç•¥ä¼°è®¡
                for i in range(0, len(para), chars_per_chunk):
                    chunk_text = para[i:i+chars_per_chunk]
                    chunks.append(chunk_text)
            else:
                if current_chunk_tokens + para_tokens > chunk_size_tokens and current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                    current_chunk = [para]
                    current_chunk_tokens = para_tokens
                else:
                    current_chunk.append(para)
                    current_chunk_tokens += para_tokens
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        safe_print(f"         åˆ†æˆ {len(chunks)} æ®µè¿›è¡Œå­—æ®µå‹ç¼©")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context_info = ""
        if task_input:
            context_info += f"\n<ä»»åŠ¡éœ€æ±‚>\n{task_input}\n</ä»»åŠ¡éœ€æ±‚>\n"
        if thinking:
            context_info += f"\n<å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n{thinking}\n</å½“å‰è¿›åº¦ä¸è®¡åˆ’>\n"
        if field_context:
            context_info += f"\n<å­—æ®µæ¥æº>\nè¿™æ˜¯æœ€æ–°åŠ¨ä½œä¸­ {field_context} çš„å†…å®¹\n</å­—æ®µæ¥æº>\n"
        
        # å‹ç¼©æ¯ä¸ªchunk
        chunk_results = []
        target_per_chunk = target_tokens // len(chunks)
        
        for i, chunk in enumerate(chunks):
            safe_print(f"         å‹ç¼©å­—æ®µç¬¬ {i+1}/{len(chunks)} æ®µ...")
            
            prompt = f"""ä½ æ˜¯æ™ºèƒ½å†…å®¹å‹ç¼©åŠ©æ‰‹ã€‚è¿™æ˜¯åˆ†æ®µå‹ç¼©çš„ç¬¬ {i+1}/{len(chunks)} æ®µ{content_type}ã€‚

{context_info}

<æœ¬æ®µå†…å®¹>
{chunk}
</æœ¬æ®µå†…å®¹>

å‹ç¼©è¦æ±‚ï¼š
1. **ç›®æ ‡é•¿åº¦**: ä¸¥æ ¼æ§åˆ¶åœ¨ {target_per_chunk} tokens ä»¥å†…
2. **æ™ºèƒ½ç­›é€‰**: {focus}
3. **ä¼˜å…ˆä¿ç•™**: å…³é”®ä¿¡æ¯ã€é‡è¦æ•°æ®ã€æ–‡ä»¶è·¯å¾„
4. **æ ¼å¼è¦æ±‚**: ä¿æŒè¿è´¯æ€§ï¼Œä½¿ç”¨æ€»ç»“è€Œéæˆªæ–­

è¯·ç›´æ¥è¾“å‡ºæœ¬æ®µçš„å‹ç¼©ç»“æœï¼š"""
            
            history = [ChatMessage(role="user", content=prompt)]
            
            try:
                response = self.llm_client.chat(
                    history=history,
                    model=self.llm_client.compressor_models[0],
                    system_prompt=f"å‹ç¼©ä¸“å®¶ã€‚ç›®æ ‡ï¼šå°†æœ¬æ®µå‹ç¼©åˆ°{target_per_chunk} tokensã€‚",
                    tool_list=[],  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨å·¥å…·
                    tool_choice="none"  # æ˜ç¡®è¡¨ç¤ºä¸è°ƒç”¨å·¥å…·ï¼ˆå‹ç¼©ä»»åŠ¡ï¼‰
                )
                
                if response.status == "success":
                    chunk_results.append(response.output)
                    safe_print(f"            âœ… ç¬¬{i+1}æ®µå‹ç¼©æˆåŠŸ")
                else:
                    chunk_results.append(chunk[:500] + "\n[æœ¬æ®µå‹ç¼©å¤±è´¥]")
                    safe_print(f"            âš ï¸ ç¬¬{i+1}æ®µå‹ç¼©å¤±è´¥")
            except Exception as e:
                chunk_results.append(chunk[:500] + "\n[æœ¬æ®µå‹ç¼©å¼‚å¸¸]")
                safe_print(f"            âŒ ç¬¬{i+1}æ®µå‹ç¼©å¼‚å¸¸: {e}")
        
        # åˆå¹¶ç»“æœ
        final_result = '\n\n---\n\n'.join(chunk_results)
        
        safe_print(f"         âœ… å­—æ®µåˆ†æ®µå‹ç¼©å®Œæˆï¼Œå…±{len(chunks)}æ®µ")
        
        return final_result
    
    def _fallback_compress(self, text: str, max_tokens: int) -> str:
        """
        å¤‡ç”¨å‹ç¼©æ–¹æ¡ˆï¼ˆé¦–å°¾ä¿ç•™æ³•ï¼‰- å½“LLMå‹ç¼©å¤±è´¥æ—¶ä½¿ç”¨
        """
        if self.encoding:
            tokens = self.encoding.encode(text)
            head_count = int(max_tokens * 0.1)
            tail_count = int(max_tokens * 0.1)
            head_tokens = tokens[:head_count]
            tail_tokens = tokens[-tail_count:]
            head_text = self.encoding.decode(head_tokens)
            tail_text = self.encoding.decode(tail_tokens)
            omitted = len(tokens) - head_count - tail_count
            return f"{head_text}\n\n[ä¸­é—´çœç•¥çº¦{omitted}ä¸ªtokens]\n\n{tail_text}"
        else:
            # ç®€å•å­—ç¬¦æˆªå–
            chars = int(max_tokens * 2)
            head = chars // 2
            tail = chars // 2
            return f"{text[:head]}\n\n[ä¸­é—´çœç•¥]\n\n{text[-tail:]}"


if __name__ == "__main__":
    safe_print("âœ… ActionCompressoræ¨¡å—åŠ è½½æˆåŠŸ")
    safe_print("\nå‹ç¼©ç­–ç•¥ï¼š")
    safe_print("1. å†å²actions â†’ LLMæ€»ç»“ä¸º5k tokens")
    safe_print("2. æœ€æ–°action â†’ ä¿ç•™ç»“æ„ï¼ŒLLMæ™ºèƒ½å‹ç¼©å¤§å­—æ®µåˆ°50% max_window")
    safe_print("3. å¤‡ç”¨æ–¹æ¡ˆ â†’ é¦–å°¾ä¿ç•™æ³•ï¼ˆå½“LLMå¤±è´¥æ—¶ï¼‰")

